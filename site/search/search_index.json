{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AfriSenti-SemEval Shared Task 12 AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset Contact organizers at: afrisenti-semeval-organizers@googlegroups.com Join Task Slack Channel to communicate with the organizers. Motivation Due to the widespread use of the Internet and social media platforms, most languages are becoming digitally available. This allows for various artificial intelligence (AI) applications that enable tasks such as sentiment analysis, machine translation and hateful content detection. According to UNESCO (2003), 30% of all living languages, around 2,058, are African languages. However, most of these languages do not have curated datasets for developing such AI applications. Recently, various individual and funded initiatives, such as the Lacuna Fund, have set out to reverse this trend and create such datasets for African languages. However, research is required to determine both the suitability of current natural language processing (NLP) techniques and the development of novel techniques to maximize the applications of such datasets. There has been a growing interest in sentiment analysis which applies to many domains, including public health, commerce/business, art and literature, social sciences, neuroscience, and psychology ( Mohammad, Saif M, 2022 ). Previous shared tasks on sentiment analysis include Mohammad, Saif M et al., (2018), Nakov et al., (2016), Pontiki et al., Ghosh et al., (2015), (2014), and so on . However, none of these tasks included African languages. Though Mohammad, Saif, et al. (2018) included standard Arabic, we focus on Arabic dialects from African countries: Algerian Arabic and Tunisian Arabizi . We believe SemEval is the right venue, due to its popularity and widespread acceptance, to carry out shared tasks for African languages to strengthen their further development. In this shared task, we have covered 13 African languages, 4 languages from Nigeria ( Hausa , Yoruba , Igbo , Nigerian Pigdin ), 2 from Ethiopia ( Amharic and Tigrinya ), Swahili from Kenya and Tanzania, Algerian Arabic dialect from Algeria, Tunisian Arabizi from Tunisia, Kinyarwanda , Twi from Ghana, and 2 languages from South Africa ( isiZulu , Setswana ). We are also working on adding more languages to the shared task. Task Overview The AfriSenti-SemEval Shared Task 12 is based on a collection of Twitter datasets in 13 African languages for sentiment classification. It consists of three sub-tasks. Participants can select one or more tasks depending on their preference. Task A: Monolingual Sentiment Classification Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. Task B: Multilingual Sentiment Classification Given a combined training data from 10 African languages, determine the polarity of a tweet in the target language (positive, negative, or neutral) Task C: Zero-Shot Sentiment Classification Given unlabeled tweets in two African languages (Tigrinya and Kinyarwanda), leverage any or all of the available training datasets in Subtasks 1 and 2 to determine the sentiment of a tweet in the two target languages is positive, negative, or neutral. Dataset Examples The dataset involves tweets labeled with three sentiment classes (positive, negative, neutral) in 12 African languages. Each tweet is annotated by three annotators following the annotation guidelines in ( Mohammad, Saif M, 2016 ). We use a form of majority vote to determine the sentiment of the tweet. See more in our paper ( Muhammad et al., 2022 , Yimam et al., 2020 ). Below is a sample dataset for the 4 Nigerian languges (Muhammad et al., 2022): The datasets are available on Github Important Dates Descriptions Deadlines Sample Data Ready 15 July 2022 Training Data Ready 1 September 2022 Evaluation Start 10 January 2023 Evaluation End 31 January 2023 System Description Paper Due February 2023 Notification to authors March 2023 Camera ready due April 2023 SemEval workshop Summer 2023 (co-located with a major NLP conference) All deadlines are 23:59 UTC-12 (\"anywhere on Earth\"). Communication Join Task Mailing List Join Task Slack Channel to communicate with the organizers. Contact Organizers: afrisenti-semeval-organizers@googlegroups.com Previous Shared Tasks Shared tasks in English: SemEval-2017 , SemEval-2016 , SemEval-2015 , SemEval-2014 , SemEval-2013 Shared tasks in Spanish TASS-2017 , TASS-2016 , TASS-2015 , TASS-2014 , TASS-2013 , TASS-2012 . References UNESCO. 2003. Sharing the world of difference. UNESCO. Mohammad, Saif M. \"Ethics sheet for automatic emotion recognition and sentiment analysis.\" Computational Linguistics 48.2 (2022): 239-278. Preslav Nakov, Sara Rosenthal, Svetlana Kiritchenko, Saif M Mohammad, Zornitsa Kozareva, Alan Ritter, Veselin Stoyanov, and Xiaodan Zhu. 2016. Developing a successful SemEval task in sentiment analysis of twitter and other social media texts. Language Resources and Evaluation, 50(1):35\u201365. Mohammad, Saif, et al. \"Semeval-2018 task 1: Affect in tweets.\" Proceedings of the 12th international workshop on semantic evaluation. 2018. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar. 2014: SemEval-2014 Task 4: Aspect Based Sentiment Analysis, Dublin, Ireland Saif Mohammad. 2016. A Practical Guide to Sentiment Annotation: Challenges and Solutions. In Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 174\u2013179, San Diego, California. Association for Computational Linguistics. Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, John Barnden, Antonio Reyes. 2015: SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter, Denver, Colorado Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Sebastian Ruder, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, Alipio Jeorge, Pavel Brazdil. 2022, NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis, Marseille, France Seid Muhie Yimam, Hizkiel Mitiku Alemayehu, Abinew Ayele, Chris Biemann. 2020: Exploring Amharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models, Barcelona, Spain (Online) body { text-align: justify}","title":"Home"},{"location":"#afrisenti-semeval-shared-task-12","text":"AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset Contact organizers at: afrisenti-semeval-organizers@googlegroups.com Join Task Slack Channel to communicate with the organizers.","title":"AfriSenti-SemEval Shared Task 12"},{"location":"#motivation","text":"Due to the widespread use of the Internet and social media platforms, most languages are becoming digitally available. This allows for various artificial intelligence (AI) applications that enable tasks such as sentiment analysis, machine translation and hateful content detection. According to UNESCO (2003), 30% of all living languages, around 2,058, are African languages. However, most of these languages do not have curated datasets for developing such AI applications. Recently, various individual and funded initiatives, such as the Lacuna Fund, have set out to reverse this trend and create such datasets for African languages. However, research is required to determine both the suitability of current natural language processing (NLP) techniques and the development of novel techniques to maximize the applications of such datasets. There has been a growing interest in sentiment analysis which applies to many domains, including public health, commerce/business, art and literature, social sciences, neuroscience, and psychology ( Mohammad, Saif M, 2022 ). Previous shared tasks on sentiment analysis include Mohammad, Saif M et al., (2018), Nakov et al., (2016), Pontiki et al., Ghosh et al., (2015), (2014), and so on . However, none of these tasks included African languages. Though Mohammad, Saif, et al. (2018) included standard Arabic, we focus on Arabic dialects from African countries: Algerian Arabic and Tunisian Arabizi . We believe SemEval is the right venue, due to its popularity and widespread acceptance, to carry out shared tasks for African languages to strengthen their further development. In this shared task, we have covered 13 African languages, 4 languages from Nigeria ( Hausa , Yoruba , Igbo , Nigerian Pigdin ), 2 from Ethiopia ( Amharic and Tigrinya ), Swahili from Kenya and Tanzania, Algerian Arabic dialect from Algeria, Tunisian Arabizi from Tunisia, Kinyarwanda , Twi from Ghana, and 2 languages from South Africa ( isiZulu , Setswana ). We are also working on adding more languages to the shared task.","title":"Motivation"},{"location":"#task-overview","text":"The AfriSenti-SemEval Shared Task 12 is based on a collection of Twitter datasets in 13 African languages for sentiment classification. It consists of three sub-tasks. Participants can select one or more tasks depending on their preference. Task A: Monolingual Sentiment Classification Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. Task B: Multilingual Sentiment Classification Given a combined training data from 10 African languages, determine the polarity of a tweet in the target language (positive, negative, or neutral) Task C: Zero-Shot Sentiment Classification Given unlabeled tweets in two African languages (Tigrinya and Kinyarwanda), leverage any or all of the available training datasets in Subtasks 1 and 2 to determine the sentiment of a tweet in the two target languages is positive, negative, or neutral.","title":"Task Overview"},{"location":"#dataset-examples","text":"The dataset involves tweets labeled with three sentiment classes (positive, negative, neutral) in 12 African languages. Each tweet is annotated by three annotators following the annotation guidelines in ( Mohammad, Saif M, 2016 ). We use a form of majority vote to determine the sentiment of the tweet. See more in our paper ( Muhammad et al., 2022 , Yimam et al., 2020 ). Below is a sample dataset for the 4 Nigerian languges (Muhammad et al., 2022): The datasets are available on Github","title":"Dataset Examples"},{"location":"#important-dates","text":"Descriptions Deadlines Sample Data Ready 15 July 2022 Training Data Ready 1 September 2022 Evaluation Start 10 January 2023 Evaluation End 31 January 2023 System Description Paper Due February 2023 Notification to authors March 2023 Camera ready due April 2023 SemEval workshop Summer 2023 (co-located with a major NLP conference) All deadlines are 23:59 UTC-12 (\"anywhere on Earth\").","title":"Important Dates"},{"location":"#communication","text":"Join Task Mailing List Join Task Slack Channel to communicate with the organizers. Contact Organizers: afrisenti-semeval-organizers@googlegroups.com","title":"Communication"},{"location":"#previous-shared-tasks","text":"Shared tasks in English: SemEval-2017 , SemEval-2016 , SemEval-2015 , SemEval-2014 , SemEval-2013 Shared tasks in Spanish TASS-2017 , TASS-2016 , TASS-2015 , TASS-2014 , TASS-2013 , TASS-2012 .","title":"Previous Shared Tasks"},{"location":"#references","text":"UNESCO. 2003. Sharing the world of difference. UNESCO. Mohammad, Saif M. \"Ethics sheet for automatic emotion recognition and sentiment analysis.\" Computational Linguistics 48.2 (2022): 239-278. Preslav Nakov, Sara Rosenthal, Svetlana Kiritchenko, Saif M Mohammad, Zornitsa Kozareva, Alan Ritter, Veselin Stoyanov, and Xiaodan Zhu. 2016. Developing a successful SemEval task in sentiment analysis of twitter and other social media texts. Language Resources and Evaluation, 50(1):35\u201365. Mohammad, Saif, et al. \"Semeval-2018 task 1: Affect in tweets.\" Proceedings of the 12th international workshop on semantic evaluation. 2018. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar. 2014: SemEval-2014 Task 4: Aspect Based Sentiment Analysis, Dublin, Ireland Saif Mohammad. 2016. A Practical Guide to Sentiment Annotation: Challenges and Solutions. In Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 174\u2013179, San Diego, California. Association for Computational Linguistics. Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, John Barnden, Antonio Reyes. 2015: SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter, Denver, Colorado Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Sebastian Ruder, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, Alipio Jeorge, Pavel Brazdil. 2022, NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis, Marseille, France Seid Muhie Yimam, Hizkiel Mitiku Alemayehu, Abinew Ayele, Chris Biemann. 2020: Exploring Amharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models, Barcelona, Spain (Online) body { text-align: justify}","title":"References"},{"location":"Resources/","text":"Resources We provide some useful information about SemEval shared task. SemEval 2023 Shared Tasks Frequently Asked Questions about SemEval Paper Submission Requirements Guidelines for Writing Papers Paper style files Paper submission site (TBD)","title":"Resources"},{"location":"Resources/#resources","text":"We provide some useful information about SemEval shared task. SemEval 2023 Shared Tasks Frequently Asked Questions about SemEval Paper Submission Requirements Guidelines for Writing Papers Paper style files Paper submission site (TBD)","title":"Resources"},{"location":"Terms/","text":"Terms and Conditions These terms and conditions are adapted from SemEval-2018 Task 1: Affect in Tweets By submitting results to this competition, you consent to the public release of your scores at this website and at SemEval-2023 workshop and in the associated proceedings, at the task organizers' discretion. Scores may include, but are not limited to, automatic and manual quantitative judgements, qualitative judgements, and such other metrics as the task organizers see fit. You accept that the ultimate decision of metric choice and score value is that of the task organizers. You further agree that the task organizers are under no obligation to release scores and that scores may be withheld if it is the task organizers' judgement that the submission was incomplete, erroneous, deceptive, or violated the letter or spirit of the competition's rules. Inclusion of a submission's scores is not an endorsement of a team or individual's submission, system, or science. A participant can be involved in exactly one team (no more). If there are reasons why it makes sense for you to be on more than one team, then email us before the evaluation period begins. In special circumstances this may be allowed. Each team must create and use exactly one CodaLab account. Team constitution (members of a team) cannot be changed after the evaluation period has begun. During the evaluation period: Each team can submit as many as fifty submissions . However, only the final submission will be considered as the official submission to the competition. You will not be able to see results of your submission on the test set. You will be able to see any warnings and errors for each of your submission. Leaderboard is disabled. Once the competition is over, we will release the gold labels and you will be able to determine results on various system variants you may have developed. We encourage you to report results on all of your systems (or system variants) in the system-description paper. However, we will ask you to clearly indicate the result of your official submission. We will make the final submissions of the teams public at some point after the evaluation period. The organizers and their affiliated institutions makes no warranties regarding the datasets provided, including but not limited to being correct or complete. They cannot be held liable for providing access to the datasets or the usage of the datasets. Each task participant will be assigned another teams\u2019 system description papers for review, using the Open Review system . The papers will thus be peer reviewed. The dataset should only be used for scientific or research purposes. Any other use is explicitly prohibited. The datasets must not be redistributed or shared in part or full with any third party. Redirect interested parties to this website. If you use any of the datasets provided here, cite this paper: Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Sebastian Ruder, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, Alipio Jeorge, Pavel Brazdil. 2022, NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis, Marseille, France Seid Muhie Yimam, Hizkiel Mitiku Alemayehu, Abinew Ayele, Chris Biemann. 2020: Exploring Amharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models, Barcelona, Spain (Online) body { text-align: justify}","title":"Terms"},{"location":"Terms/#terms-and-conditions","text":"These terms and conditions are adapted from SemEval-2018 Task 1: Affect in Tweets By submitting results to this competition, you consent to the public release of your scores at this website and at SemEval-2023 workshop and in the associated proceedings, at the task organizers' discretion. Scores may include, but are not limited to, automatic and manual quantitative judgements, qualitative judgements, and such other metrics as the task organizers see fit. You accept that the ultimate decision of metric choice and score value is that of the task organizers. You further agree that the task organizers are under no obligation to release scores and that scores may be withheld if it is the task organizers' judgement that the submission was incomplete, erroneous, deceptive, or violated the letter or spirit of the competition's rules. Inclusion of a submission's scores is not an endorsement of a team or individual's submission, system, or science. A participant can be involved in exactly one team (no more). If there are reasons why it makes sense for you to be on more than one team, then email us before the evaluation period begins. In special circumstances this may be allowed. Each team must create and use exactly one CodaLab account. Team constitution (members of a team) cannot be changed after the evaluation period has begun. During the evaluation period: Each team can submit as many as fifty submissions . However, only the final submission will be considered as the official submission to the competition. You will not be able to see results of your submission on the test set. You will be able to see any warnings and errors for each of your submission. Leaderboard is disabled. Once the competition is over, we will release the gold labels and you will be able to determine results on various system variants you may have developed. We encourage you to report results on all of your systems (or system variants) in the system-description paper. However, we will ask you to clearly indicate the result of your official submission. We will make the final submissions of the teams public at some point after the evaluation period. The organizers and their affiliated institutions makes no warranties regarding the datasets provided, including but not limited to being correct or complete. They cannot be held liable for providing access to the datasets or the usage of the datasets. Each task participant will be assigned another teams\u2019 system description papers for review, using the Open Review system . The papers will thus be peer reviewed. The dataset should only be used for scientific or research purposes. Any other use is explicitly prohibited. The datasets must not be redistributed or shared in part or full with any third party. Redirect interested parties to this website. If you use any of the datasets provided here, cite this paper: Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Sebastian Ruder, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris Chinenye Emezue, Saheed Salahudeen Abdullahi, Anuoluwapo Aremu, Alipio Jeorge, Pavel Brazdil. 2022, NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis, Marseille, France Seid Muhie Yimam, Hizkiel Mitiku Alemayehu, Abinew Ayele, Chris Biemann. 2020: Exploring Amharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models, Barcelona, Spain (Online) body { text-align: justify}","title":"Terms and Conditions"},{"location":"about/","text":"","title":"About"},{"location":"award/","text":"Awards Best Paper (task participants): Best Task Award TASK A: Monolingual Sentiment Classification TASK B: Sentiment Multilingual Sentiment Classification TASK C: Zero-Shot Sentiment Classification TASK D: Sentiment Classification in Multiple Languages TASK E: Sentiment Classification in Multiple Languages with Multiple Annotators","title":"Awards"},{"location":"award/#awards","text":"Best Paper (task participants): Best Task Award TASK A: Monolingual Sentiment Classification TASK B: Sentiment Multilingual Sentiment Classification TASK C: Zero-Shot Sentiment Classification TASK D: Sentiment Classification in Multiple Languages TASK E: Sentiment Classification in Multiple Languages with Multiple Annotators","title":"Awards"},{"location":"datasets/","text":"Competition Dataset The dataset for the competition is available at the following Github link: AfriSenti-SemEval Dataset The dataset consists of tweets labeled with three sentiment classes (positive, negative, neutral) in 12 African languages. Each tweet is annotated by three annotators. We use a form of majority vote to determine the sentiment of the tweet. See more in our paper: Below is a sample dataset in all the languages: Dataset Statistics Below is a statistic of the dataset in all the 12 languages. Languages Train Validation Test sXYZ sBlu z Jaobe X Blue 5.2 sXYZ Jaobe X Trail Data The trial data is available here. Training Data The training data is available here. Hyrdating Tweets table { border-collapse: collapse; } table, th, td { border: 1px solid black; } blockquote { border-left: solid blue; padding-left: 10px; }","title":"Competition Dataset"},{"location":"datasets/#competition-dataset","text":"The dataset for the competition is available at the following Github link: AfriSenti-SemEval Dataset The dataset consists of tweets labeled with three sentiment classes (positive, negative, neutral) in 12 African languages. Each tweet is annotated by three annotators. We use a form of majority vote to determine the sentiment of the tweet. See more in our paper: Below is a sample dataset in all the languages:","title":"Competition Dataset"},{"location":"datasets/#dataset-statistics","text":"Below is a statistic of the dataset in all the 12 languages. Languages Train Validation Test sXYZ sBlu z Jaobe X Blue 5.2 sXYZ Jaobe X","title":"Dataset Statistics"},{"location":"datasets/#trail-data","text":"The trial data is available here.","title":"Trail Data"},{"location":"datasets/#training-data","text":"The training data is available here.","title":"Training Data"},{"location":"datasets/#hyrdating-tweets","text":"table { border-collapse: collapse; } table, th, td { border: 1px solid black; } blockquote { border-left: solid blue; padding-left: 10px; }","title":"Hyrdating Tweets"},{"location":"organizer/","text":"Task Organizers Below is a list of organizers for AfriSenti-Shared Task 2022 Organizers Affliation Shamsuddeen Hassan Muhammad Bayero University, Kano, Masakhane Seid Muhie Yimam Universit\u00e4t Hamburg, Hamburg; Masakhane Idris Abdulmumin Ahmadu Bello UniversityZaria, Masakhane Ibrahim Sa\u2019id Ahmad Bayero University, Kano Abinew Ali Ayele Bahir Dar University, Bahir Dar David Ifeoluwa Adelani Saarland University, MasaKhane Vukosi Marivate University of Pretoria; MasaKhane Sebastian Ruder Google Research Saif M. Mohammad National Research Council Canada Nedjma Ousidhoum The University of Cambridge Meriem Beloucif Uppsala University Tadesse Destaw Belay Wollo University, Dessie","title":"Organizers"},{"location":"organizer/#task-organizers","text":"Below is a list of organizers for AfriSenti-Shared Task 2022 Organizers Affliation Shamsuddeen Hassan Muhammad Bayero University, Kano, Masakhane Seid Muhie Yimam Universit\u00e4t Hamburg, Hamburg; Masakhane Idris Abdulmumin Ahmadu Bello UniversityZaria, Masakhane Ibrahim Sa\u2019id Ahmad Bayero University, Kano Abinew Ali Ayele Bahir Dar University, Bahir Dar David Ifeoluwa Adelani Saarland University, MasaKhane Vukosi Marivate University of Pretoria; MasaKhane Sebastian Ruder Google Research Saif M. Mohammad National Research Council Canada Nedjma Ousidhoum The University of Cambridge Meriem Beloucif Uppsala University Tadesse Destaw Belay Wollo University, Dessie","title":"Task Organizers"},{"location":"participation/","text":"How to Participate The organisers have defined three-subtasks and will release the training data. Participants can work on any or all of the subtasks. The participants can form a team with multiple people, or a single person team is okay. The participants can experiment with the training data to develop models. Usage of any external data or resources is allowed and highly encouraged. This process can run util the evaluation period. Evaluation period: Organisers will release the test set containing instances without the labels. The participants will use their developed models to predict the labels for the instances, and they have to create a submission file that follows exactly the same format as the training data. These prediction files should be submitted to Codalab submission portal (will be announced later). These predictions will be compared against the ground truth labels of the test data and the teams will be ranked on a leaderboard according to the performance score. Each team is encouraged to write a system description paper describing their submission system, including interesting insights, and submit it before the due date. After a review period, each team has to update their submitted paper based on the review feedback and submit the camera-ready version. Accepted papers will be published as part of the proceedings of the SemEval 2023 Workshop (link). Trial Data We provide a sample dataset for the shared task. Check the Github repository . Training Data coming soon Submission Website We will be accepting and evaluating your submissions on CodaLab . The submission page will be available soon! Evaluation Since our datasets are imbalance, we will use macro-F1 (macro-averaged F1) score for model evaluation. Results The Evaluation phase of Afri-Senti-SemEval 2023 will start on January 10, 2023 and end on 31 January 2023. The Evaluation results and rankings will be notified in this page. We will list the usernames as they appear on CodaLab and the F1 score of your LAST submission. System Description Papers Participants who made a submission on the CodaLab website during the official evaluation period are given the opportunity to submit a system-description paper. The paper will be part of the SemEval-2023 proceedings. body { text-align: justify}","title":"Participation"},{"location":"participation/#how-to-participate","text":"The organisers have defined three-subtasks and will release the training data. Participants can work on any or all of the subtasks. The participants can form a team with multiple people, or a single person team is okay. The participants can experiment with the training data to develop models. Usage of any external data or resources is allowed and highly encouraged. This process can run util the evaluation period. Evaluation period: Organisers will release the test set containing instances without the labels. The participants will use their developed models to predict the labels for the instances, and they have to create a submission file that follows exactly the same format as the training data. These prediction files should be submitted to Codalab submission portal (will be announced later). These predictions will be compared against the ground truth labels of the test data and the teams will be ranked on a leaderboard according to the performance score. Each team is encouraged to write a system description paper describing their submission system, including interesting insights, and submit it before the due date. After a review period, each team has to update their submitted paper based on the review feedback and submit the camera-ready version. Accepted papers will be published as part of the proceedings of the SemEval 2023 Workshop (link).","title":"How to Participate"},{"location":"participation/#trial-data","text":"We provide a sample dataset for the shared task. Check the Github repository .","title":"Trial Data"},{"location":"participation/#training-data","text":"coming soon","title":"Training Data"},{"location":"participation/#submission-website","text":"We will be accepting and evaluating your submissions on CodaLab . The submission page will be available soon!","title":"Submission Website"},{"location":"participation/#evaluation","text":"Since our datasets are imbalance, we will use macro-F1 (macro-averaged F1) score for model evaluation.","title":"Evaluation"},{"location":"participation/#results","text":"The Evaluation phase of Afri-Senti-SemEval 2023 will start on January 10, 2023 and end on 31 January 2023. The Evaluation results and rankings will be notified in this page. We will list the usernames as they appear on CodaLab and the F1 score of your LAST submission.","title":"Results"},{"location":"participation/#system-description-papers","text":"Participants who made a submission on the CodaLab website during the official evaluation period are given the opportunity to submit a system-description paper. The paper will be part of the SemEval-2023 proceedings. body { text-align: justify}","title":"System Description Papers"},{"location":"prizes/","text":"Prizes AfriSenti-SemEval competition has a prize and will be awarded to best three performing teams in each sub-task Details is coming soon.","title":"Awards"},{"location":"prizes/#prizes","text":"AfriSenti-SemEval competition has a prize and will be awarded to best three performing teams in each sub-task Details is coming soon.","title":"Prizes"}]}